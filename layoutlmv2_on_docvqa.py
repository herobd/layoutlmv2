# -*- coding: utf-8 -*-
"""Copy of Fine-tuning LayoutLMv2ForQuestionAnswering on DocVQA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A8zUtBixqN5TjvmNDR1ytENkQ8Je-956

## Fine-tuning LayoutLMv2ForQuestionAnswering on DocVQA

In this notebook, we are going to fine-tune LayoutLMv2 on the DocVQA dataset to perform visual question answering on document images. As stated in the paper, the authors treat the problem similar to how BERT treats the SQuAD dataset, i.e. by adding a question answering head on top of the final hidden states of the tokens, in order to predict which token is at the start of the answer and which token is at the end of the answer. In other words, the problem is treated as extractive question answering: given the context, extract which piece of information answers the question.

In this case, the context will come from the output of an OCR engine, like Google's Tesseract.

NOTE: in order to understand `LayoutLMv2ForQuestionAnswering`, it is required to understand `BertForQuestionAnswering`. I highly recommend [this guide](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/) by Chris McCormick, which explains the model in detail.

* LayoutLMv2 paper: https://arxiv.org/abs/2012.14740
* DocVQA paper: https://arxiv.org/abs/2007.00398
* DocVQA website: https://docvqa.org/
* DocVQA leaderboard (from which you can download the data after registring): https://rrc.cvc.uab.es/?ch=17

Also, the documentation of LayoutLMv2 can be found [here](https://huggingface.co/transformers/model_doc/layoutlmv2.html).

## Install dependencies

First, we install all dependencies:

* HuggingFace Transformers + Detectron2 (for the LayoutLMv2 model).
* HuggingFace Datasets (for data processing).
* PyTesseract (OCR engine, which we are going to use to get the words + bounding boxes of document images).
"""

#!pip install -q transformers

#!pip install pyyaml==5.1
# workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158)
#!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html

# install detectron2 that matches pytorch 1.8
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
#!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html
# exit(0)  # After installation, you need to "restart runtime" in Colab. This line can also restart runtime

#!pip install -q datasets

#!sudo apt install tesseract-ocr
#!pip install -q pytesseract

"""We first define some general variables, which we can easily tweak."""

model_checkpoint = "pairing/cache_huggingface/layoutlmv2-base-uncased"
batch_size = 16

"""## Analysis

Let's load the DocVQA validation split. You can download the DocVQA data (after registration) [here](https://rrc.cvc.uab.es/?ch=17).
"""

import json

with open('data/DocVQA/val/val_v1.0.json') as f:
  data = json.load(f)

data.keys()

print("Dataset name:", data['dataset_name'])
print("Dataset split:", data['dataset_split'])

"""Let's read in the entire dataset of question + answer pairs:"""

import pandas as pd

df = pd.DataFrame(data['data'])
df.head()

# pick a random example
#example = data['data'][10]
#for k,v in example.items():
#  print(k + ":", v)

from PIL import Image

root_dir = 'data/DocVQA/val/'
#image = Image.open(root_dir + example['image'])
#image

#"""Let's get the corresponding words:"""

#ocr_root_dir = root_dir + "ocr_results/"

#with open(ocr_root_dir + example['ucsf_document_id'] + "_" + example['ucsf_document_page_no'] + ".json") as f:
#  ocr = json.load(f)

#ocr.keys()

#len(ocr['recognitionResults'][0]['lines'])

#words = ""
#for item in ocr['recognitionResults'][0]['lines']:
#  words += item['text']

#print(words)

"""## Apply OCR on the entire dataset

The DocVQA comes with annotated OCR results, but we are going to use `LayoutLMv2FeatureExtractor` (which uses Google's Tesseract under the hood) to prepare the images for the model, and get the words and normalized boxes. We will use HuggingFace Datasets to process the entire dataset in a fast way.
"""

from datasets import Dataset

print("TESTING: only using 10 instances")
dataset = Dataset.from_pandas(df.iloc[:10])

#dataset[0]

#len(dataset)

from transformers import LayoutLMv2FeatureExtractor

feature_extractor = LayoutLMv2FeatureExtractor()

def get_ocr_words_and_boxes(examples):
    
  # get a batch of document images
  images = [Image.open(root_dir + image_file).convert("RGB") for image_file in examples['image']]
  
  # resize every image to 224x224 + apply tesseract to get words + normalized boxes
  encoded_inputs = feature_extractor(images)

  examples['image'] = encoded_inputs.pixel_values
  examples['words'] = encoded_inputs.words
  examples['boxes'] = encoded_inputs.boxes

  return examples

dataset_with_ocr = dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)

# check whether words and normalized bounding boxes are added correctly
#print(dataset_with_ocr[0]['words'])
#print(dataset_with_ocr[0]['boxes'])
#print("-----")
#print(dataset_with_ocr[1]['words'])
#print(dataset_with_ocr[1]['boxes'])

#dataset_with_ocr[0].keys()

"""## Encode the dataset

Next, we can encode the dataset, i.e. prepare it for the model.

This involves converting the words and boxes to token-level `input_ids`, `attention_mask`, `token_type_ids` and `bbox`. 

Besides that, we need to add the labels for the model. For `xxxForQuestionAnswering` models in HuggingFace Transformers, the labels consist of the `start_positions` and `end_positions`, indicating which token is at the start and which token is at the end of the answer. 

For this, we can use the `word_ids` of `LayoutLMv2TokenizerFast`, which indicate to which original word each token belongs.

Let's show with a small example how we can create the `start_positions` and `end_positions`. We make use of a function called `subfinder`, which allows us to find a sublist in a larger list.
"""

# source: https://stackoverflow.com/a/12576755
def subfinder(words_list, answer_list):  
    matches = []
    start_indices = []
    end_indices = []
    for idx, i in enumerate(range(len(words_list))):
        if words_list[i] == answer_list[0] and words_list[i:i+len(answer_list)] == answer_list:
            matches.append(answer_list)
            start_indices.append(idx)
            end_indices.append(idx + len(answer_list) - 1)
    if matches:
      return matches[0], start_indices[0], end_indices[0]
    else:
      return None, 0, 0

## example
#question = "where is it located?"
#words = ["this", "is", "located", "in", "the", "university", "of", "california", "in", "the", "US"]
#boxes = [[1000,1000,1000,1000] for _ in range(len(words))]
#answer = "university of california"
#
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
#
import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)
#
## encode the example
#encoding = tokenizer(question, words, boxes=boxes)
#
#tokenizer.decode(encoding.input_ids)
#
#print(encoding.word_ids())
#
#print(encoding.sequence_ids())
#
## find the answer in the words
#match, word_idx_start, word_idx_end = subfinder(words, answer.split())
#
#print("Match:", match)
#print("Word idx start:", word_idx_start)
#print("Word idx end:", word_idx_end)
#
#"""Given that we've found the answer in terms of word indices of the context, we can now simply loop over the context tokens, and if the `word_id` of a token corresponds to the `word_idx_start`, than we can set its index as the `start_position`. Similarly, if the `word_id` of a token corresponds to the `word_idx_end`, than we can set its index as the `end_position`."""
#
#sequence_ids = encoding.sequence_ids()
#
## Start token index of the current span in the text.
#token_start_index = 0
#while sequence_ids[token_start_index] != 1:
#    token_start_index += 1
#
## End token index of the current span in the text.
#token_end_index = len(encoding.input_ids) - 1
#while sequence_ids[token_end_index] != 1:
#    token_end_index -= 1
#
#print("Token start index:", token_start_index)
#print("Token end index:", token_end_index)
#print(tokenizer.decode(encoding.input_ids[token_start_index:token_end_index+1]))
#
#word_ids = encoding.word_ids()[token_start_index:token_end_index+1]
#print("Word ids:", word_ids)
#for id in word_ids:
#  if id == word_idx_start:
#    start_position = token_start_index 
#  else:
#    token_start_index += 1
#
#for id in word_ids[::-1]:
#  if id == word_idx_end:
#    end_position = token_end_index 
#  else:
#    token_end_index -= 1
#
#print(start_position)
#print(end_position)
#print("Reconstructed answer:", tokenizer.decode(encoding.input_ids[start_position:end_position+1]))
#
#"""Now that we've shown how to do this with a single example, we can apply this on the entire dataset. 
#
#
#"""

def encode_dataset(examples, max_length=512):
  # take a batch 
  questions = examples['question']
  words = examples['words']
  boxes = examples['boxes']

  # encode it
  encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)

  # next, add start_positions and end_positions
  start_positions = []
  end_positions = []
  answers = examples['answers']
  # for every example in the batch:
  for batch_index in range(len(answers)):
    print("Batch index:", batch_index)
    cls_index = encoding.input_ids[batch_index].index(tokenizer.cls_token_id)
    # try to find one of the answers in the context, return first match
    words_example = [word.lower() for word in words[batch_index]]
    for answer in answers[batch_index]:
      match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())
      if match:
        break
    # EXPERIMENT (to account for when OCR context and answer don't perfectly match):
    if not match:
      for answer in answers[batch_index]:
        for i in range(len(answer)):
          # drop the ith character from the answer
          answer_i = answer[:i] + answer[i+1:]
          # check if we can find this one in the context
          #TODO make this better
          match, word_idx_start, word_idx_end = subfinder(words_example, answer_i.lower().split())
          if match:
            break
    # END OF EXPERIMENT 
    
    if match:
      sequence_ids = encoding.sequence_ids(batch_index)
      # Start token index of the current span in the text.
      token_start_index = 0
      while sequence_ids[token_start_index] != 1:
          token_start_index += 1

      # End token index of the current span in the text.
      token_end_index = len(encoding.input_ids[batch_index]) - 1
      while sequence_ids[token_end_index] != 1:
          token_end_index -= 1
      
      word_ids = encoding.word_ids(batch_index)[token_start_index:token_end_index+1]
      for id in word_ids:
        if id == word_idx_start:
          start_positions.append(token_start_index)
          break
        else:
          token_start_index += 1

      for id in word_ids[::-1]:
        if id == word_idx_end:
          end_positions.append(token_end_index)
          break
        else:
          token_end_index -= 1
      
      print("Verifying start position and end position:")
      print("True answer:", answer)
      start_position = start_positions[batch_index]
      end_position = end_positions[batch_index]
      reconstructed_answer = tokenizer.decode(encoding.input_ids[batch_index][start_position:end_position+1])
      print("Reconstructed answer:", reconstructed_answer)
      print("-----------")
    
    else:
      print("Answer not found in context")
      print("-----------")
      start_positions.append(cls_index)
      end_positions.append(cls_index)
  
  encoding['image'] = examples['image']
  encoding['start_positions'] = start_positions
  encoding['end_positions'] = end_positions

  return encoding

from datasets import Features, Sequence, Value, Array2D, Array3D

# we need to define custom features
features = Features({
    'input_ids': Sequence(feature=Value(dtype='int64')),
    'bbox': Array2D(dtype="int64", shape=(512, 4)),
    'attention_mask': Sequence(Value(dtype='int64')),
    'token_type_ids': Sequence(Value(dtype='int64')),
    'image': Array3D(dtype="int64", shape=(3, 224, 224)),
    'start_positions': Value(dtype='int64'),
    'end_positions': Value(dtype='int64'),
})

encoded_dataset = dataset_with_ocr.map(encode_dataset, batched=True, batch_size=2, 
                                       remove_columns=dataset_with_ocr.column_names,
                                       features=features)

#encoded_dataset
#import pdb;pdb.set_trace()

#TODO save here

"""Let's verify an example:"""

#idx = 44

#tokenizer.decode(encoded_dataset['input_ids'][idx])

#aimage = Image.open('/content/drive/MyDrive/LayoutLMv2/Tutorial notebooks/DocVQA/val/' + dataset['image'][idx])
#image

#start_position = encoded_dataset['start_positions'][idx]
#end_position = encoded_dataset['end_positions'][idx]
#if start_position != 0:
#  print(tokenizer.decode(encoded_dataset['input_ids'][idx][start_position: end_position+1]))
#else:
#  print("Answer not found in context")

#len(encoded_dataset)

#encoded_dataset.features

"""We can now set the format to PyTorch tensors, and create dataloaders."""

import torch

encoded_dataset.set_format(type="torch")
dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=4)
batch = next(iter(dataloader))

#for k,v in batch.items():
"""## Inference

After training, you can perform inference as follows:

1. Take an image + question, prepare it for the model using `LayoutLMv2Processor`. The processor will apply the feature extractor and tokenizer in a sequence, to get all required inputs you need.
2. Forward it through the model.
3. The model returns `start_logits` and `end_logits`, which indicate which token is at the start of the answer and which token is at the end of the answer. Both have shape (batch_size, sequence_length).
4. You can take an argmax on the last dimension of both the `start_logits` and `end_logits` to get the predicted `start_idx` and `end_idx`.
5. You can then decode the answer as follows: `processor.tokenizer.decode(input_ids[start_idx:end_idx+1])`.


"""

## step 1: pick a random example
#example = data['data'][10]
#root_dir = '/content/drive/MyDrive/LayoutLMv2/Tutorial notebooks/DocVQA/val/'
#question = example['question']
#image = Image.open(root_dir + example['image']).convert("RGB")
#print(question)
#image

from transformers import LayoutLMv2Processor

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

def run(image,question):
    # prepare for the model
    encoding = processor(image, question, return_tensors="pt")
    #print(encoding.keys())

    """Note that you can also verify what the processor has created, by decoding the `input_ids` back to text:"""

    #print(processor.tokenizer.decode(encoding.input_ids.squeeze()))

    # step 2: forward pass

    for k,v in encoding.items():
      encoding[k] = v.to(model.device)

    outputs = model(**encoding)

    # step 3: get start_logits and end_logits
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    # step 4: get largest logit for both
    predicted_start_idx = start_logits.argmax(-1).item()
    predicted_end_idx = end_logits.argmax(-1).item()
    #print("Predicted start idx:", predicted_start_idx)
    #print("Predicted end idx:", predicted_end_idx)

    # step 5: decode the predicted answer
    return processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx:predicted_end_idx+1])
#  print(k, v.shape)

#idx = 2

#tokenizer.decode(batch['input_ids'][2])

#start_position = batch['start_positions'][idx].item()
#end_position = batch['end_positions'][idx].item()

#tokenizer.decode(batch['input_ids'][idx][start_position:end_position+1])

"""## Train a model

Next, we can fine-tune the model on our dataset.
"""

from transformers import AutoModelForQuestionAnswering

model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

model.train()
for epoch in range(20):  # loop over the dataset multiple times
   for idx, batch in enumerate(dataloader):
        # get the inputs;
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        token_type_ids = batch["token_type_ids"].to(device)
        bbox = batch["bbox"].to(device)
        image = batch["image"].to(device)
        start_positions = batch["start_positions"].to(device)
        end_positions = batch["end_positions"].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                       bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)
        loss = outputs.loss
        print("Loss:", loss.item())
        loss.backward()
        optimizer.step()
    
    for idx, batch in enumerate(valid_dataloader):


"""Note that in this case, the predicted start idx and end idx don't seem to result in a reasonable answer (it just predicts a really long chunk of text). This is because we have barely trained the model, it was just for demonstration purposes.

To get really good results, you need to train on the entire training split of DocVQA, before testing on the validation set.
"""

